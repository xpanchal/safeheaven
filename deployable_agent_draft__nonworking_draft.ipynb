{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ca87a7-6d77-473b-b664-ffcd8c537ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -qqqq mlflow databricks-langchain databricks-agents uv langgraph==0.3.4 langchain_mcp_adapters\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1784b0ed-80f3-4d1a-a0f9-09f0d8ef1747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "# from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "# # get_tools is not needed here anymore as we call the client directly\n",
    "# # from get_tools import get_nimble_tools\n",
    "# import asyncio\n",
    "# import mlflow\n",
    "# from databricks_langchain import (\n",
    "#     ChatDatabricks,\n",
    "#     UCFunctionToolkit,\n",
    "#     VectorSearchRetrieverTool,\n",
    "# )\n",
    "# from langchain_core.language_models import LanguageModelLike\n",
    "# from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "# from langchain_core.tools import BaseTool\n",
    "# from langgraph.graph import END, StateGraph\n",
    "# from langgraph.graph.graph import CompiledGraph\n",
    "# from langgraph.graph.state import CompiledStateGraph\n",
    "# from langgraph.prebuilt.tool_node import ToolNode\n",
    "# from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "# from mlflow.pyfunc import ChatAgent\n",
    "# from mlflow.types.agent import (\n",
    "#     ChatAgentChunk,\n",
    "#     ChatAgentMessage,\n",
    "#     ChatAgentResponse,\n",
    "#     ChatContext,\n",
    "# )\n",
    "\n",
    "# from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "# # NOTE: The creation of the agent and its dependencies are now inside\n",
    "# # the async create_agent() function.\n",
    "\n",
    "# def create_tool_calling_agent(\n",
    "#     model: LanguageModelLike,\n",
    "#     tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "#     system_prompt: Optional[str] = None,\n",
    "# ) -> CompiledGraph:\n",
    "#     model = model.bind_tools(tools)\n",
    "\n",
    "#     def should_continue(state: ChatAgentState):\n",
    "#         messages = state[\"messages\"]\n",
    "#         last_message = messages[-1]\n",
    "#         if last_message.get(\"tool_calls\"):\n",
    "#             return \"continue\"\n",
    "#         else:\n",
    "#             return \"end\"\n",
    "\n",
    "#     if system_prompt:\n",
    "#         preprocessor = RunnableLambda(\n",
    "#             lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "#             + state[\"messages\"]\n",
    "#         )\n",
    "#     else:\n",
    "#         preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "#     model_runnable = preprocessor | model\n",
    "\n",
    "#     def call_model(\n",
    "#         state: ChatAgentState,\n",
    "#         config: RunnableConfig,\n",
    "#     ):\n",
    "#         response = model_runnable.invoke(state, config)\n",
    "#         return {\"messages\": [response]}\n",
    "\n",
    "#     workflow = StateGraph(ChatAgentState)\n",
    "#     workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "#     workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "#     workflow.set_entry_point(\"agent\")\n",
    "#     workflow.add_conditional_edges(\n",
    "#         \"agent\", should_continue, {\"continue\": \"tools\", \"end\": END}\n",
    "#     )\n",
    "#     workflow.add_edge(\"tools\", \"agent\")\n",
    "#     return workflow.compile()\n",
    "\n",
    "\n",
    "# class LangGraphChatAgent(ChatAgent):\n",
    "#     def __init__(self, agent: CompiledStateGraph):\n",
    "#         self.agent = agent\n",
    "\n",
    "#     # This remains a synchronous function\n",
    "#     # def predict(\n",
    "#     #     self,\n",
    "#     #     messages: list[ChatAgentMessage],\n",
    "#     #     context: Optional[ChatContext] = None,\n",
    "#     #     custom_inputs: Optional[dict[str, Any]] = None,\n",
    "#     # ) -> ChatAgentResponse:\n",
    "#     #     request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "#     #     response_messages = []\n",
    "#     #     for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "#     #         for node_data in event.values():\n",
    "#     #             response_messages.extend(\n",
    "#     #                 ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "#     #             )\n",
    "#     #     return ChatAgentResponse(messages=response_messages)\n",
    "\n",
    "#     def predict(\n",
    "#         self,\n",
    "#         messages: list[ChatAgentMessage],\n",
    "#         context: Optional[ChatContext] = None,\n",
    "#         custom_inputs: Optional[dict[str, Any]] = None,\n",
    "#     ) -> ChatAgentResponse:\n",
    "#         request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "#         response_messages = []\n",
    "#         for event in self.agent.astream(request, stream_mode=\"updates\"):\n",
    "#             for node_data in event.values():\n",
    "#                 # Check if messages are already ChatAgentMessage instances\n",
    "#                 # If they are, just extend the list directly\n",
    "#                 # Otherwise, assume they are dictionaries and create ChatAgentMessage objects\n",
    "#                 if node_data.get(\"messages\") and isinstance(node_data[\"messages\"][0], ChatAgentMessage):\n",
    "#                     response_messages.extend(node_data[\"messages\"])\n",
    "#                 else:\n",
    "#                     response_messages.extend(\n",
    "#                         ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "#                     )\n",
    "#         return ChatAgentResponse(messages=response_messages)\n",
    "\n",
    "#     def predict_stream(\n",
    "#         self,\n",
    "#         messages: list[ChatAgentMessage],\n",
    "#         context: Optional[ChatContext] = None,\n",
    "#         custom_inputs: Optional[dict[str, Any]] = None,\n",
    "#     ) -> Generator[ChatAgentChunk, None, None]:\n",
    "#         request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "#         for event in self.agent.astream(request, stream_mode=\"updates\"):\n",
    "#             for node_data in event.values():\n",
    "#                 yield from (\n",
    "#                     ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "#                 )\n",
    "\n",
    "# # NEW ASYNC INITIALIZATION FUNCTION\n",
    "# async def create_agent():\n",
    "#     \"\"\"\n",
    "#     An asynchronous function to initialize and return the agent.\n",
    "#     This safely handles the async call to get tools.\n",
    "#     \"\"\"\n",
    "#     # LLM and system prompt setup\n",
    "#     # llm = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\")\n",
    "#     # llm = ChatDatabricks(endpoint=\"databricks-claude-3-7-sonnet\")\n",
    "#     llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "\n",
    "\n",
    "    \n",
    "#     system_prompt = \"\"\n",
    "\n",
    "#     # Initialize tools list\n",
    "#     tools = []\n",
    "#     uc_tool_names = [\"system.ai.python_exec\"]\n",
    "#     uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "#     tools.extend(uc_toolkit.tools)\n",
    "\n",
    "#     # Nimble MCP Tools setup\n",
    "#     nimble_client = MultiServerMCPClient({\n",
    "#         \"nimble\": {\n",
    "#             \"url\": \"https://mcp.nimbleway.com/sse\",\n",
    "#             \"transport\": \"sse\",\n",
    "#             \"headers\": {\n",
    "#                 \"Authorization\": f\"Bearer 1c7cdd787c154fe3a78ebcc3428d82c48f4fada38e0042c6a427fbe7e3b842ae\"\n",
    "#             }\n",
    "#         }\n",
    "#     })\n",
    "    \n",
    "#     # *** THIS IS THE KEY CHANGE ***\n",
    "#     # We now 'await' this call inside our async function instead of using asyncio.run()\n",
    "#     nimble_tools = await nimble_client.get_tools()\n",
    "#     tools.extend(nimble_tools)\n",
    "\n",
    "#     # Create and compile the agent graph\n",
    "#     compiled_agent_graph = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "    \n",
    "#     # Instantiate our final agent object\n",
    "#     agent = LangGraphChatAgent(compiled_agent_graph)\n",
    "    \n",
    "#     # Set the model for MLflow (optional, but good practice)\n",
    "#     # mlflow.models.set_model(agent)\n",
    "\n",
    "#     return agent\n",
    "\n",
    "# # We NO LONGER create a global AGENT instance here.\n",
    "# # AGENT = ...\n",
    "\n",
    "\n",
    "# %%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import uuid # Add this import for generating IDs if needed\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import mlflow\n",
    "import json\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "\n",
    "# Import specific message types from langchain_core.messages\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage, BaseMessage, SystemMessage\n",
    "\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import ( # <-- Ensure these are imported\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "# Add these specific imports for MLflow's tool call types\n",
    "# from mlflow.types.chat import ToolCall, FunctionCall\n",
    "\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "\n",
    "# --- Helper function to convert LangChain BaseMessage to MLflow ChatAgentMessage ---\n",
    "def _to_mlflow_chat_agent_message(lc_message: BaseMessage) -> ChatAgentMessage:\n",
    "    if isinstance(lc_message, HumanMessage):\n",
    "        return ChatAgentMessage(role=\"user\", content=lc_message.content, id=str(uuid.uuid4()))\n",
    "    elif isinstance(lc_message, AIMessage):\n",
    "        mlflow_tool_calls = []\n",
    "        if lc_message.tool_calls:\n",
    "            for lc_tc in lc_message.tool_calls:\n",
    "                # CRITICAL FIX: Construct the dictionary structure expected by MLflow's ChatAgentMessage\n",
    "                function_name = lc_tc.get(\"name\")\n",
    "                function_arguments_str = json.dumps(lc_tc.get(\"args\")) \n",
    "                tool_call_id = lc_tc.get(\"id\", str(uuid.uuid4())) # Ensure ID exists\n",
    "\n",
    "                # Construct the dictionary for the 'function' part\n",
    "                function_dict = {\n",
    "                    \"name\": function_name,\n",
    "                    \"arguments\": function_arguments_str\n",
    "                }\n",
    "                # Construct the dictionary for the full 'tool_call'\n",
    "                tool_call_dict = {\n",
    "                    \"id\": tool_call_id,\n",
    "                    \"function\": function_dict # Nested dictionary\n",
    "                }\n",
    "                mlflow_tool_calls.append(tool_call_dict) # Append the dictionary\n",
    "                \n",
    "        # Ensure content is not None, replace with empty string if it is\n",
    "        content_to_use = lc_message.content if lc_message.content is not None else \"\"\n",
    "\n",
    "        return ChatAgentMessage(\n",
    "            role=\"assistant\", \n",
    "            content=content_to_use, # Use the potentially cleaned content\n",
    "            tool_calls=mlflow_tool_calls,\n",
    "            id=str(uuid.uuid4()) # ID for the ChatAgentMessage itself\n",
    "        )\n",
    "    elif isinstance(lc_message, ToolMessage):\n",
    "        return ChatAgentMessage(\n",
    "            role=\"tool\", \n",
    "            content=lc_message.content, \n",
    "            name=lc_message.name, \n",
    "            tool_call_id=lc_message.tool_call_id, # Use the ID from the LangChain ToolMessage\n",
    "            id=str(uuid.uuid4()) # ID for the ChatAgentMessage itself\n",
    "        )\n",
    "    elif isinstance(lc_message, SystemMessage):\n",
    "        return ChatAgentMessage(role=\"system\", content=lc_message.content, id=str(uuid.uuid4()))\n",
    "    else:\n",
    "        # Fallback for any other BaseMessage type\n",
    "        # Ensure content is not None, replace with empty string if it is\n",
    "        content_to_use = lc_message.content if hasattr(lc_message, 'content') and lc_message.content is not None else str(lc_message)\n",
    "        return ChatAgentMessage(role=\"assistant\", content=content_to_use, id=str(uuid.uuid4()))\n",
    "\n",
    "\n",
    "# --- _to_langchain_base_message remains the same (it converts FROM MLflow to LangChain, which was correct previously) ---\n",
    "# This function is used when converting incoming MLflow messages to LangChain messages for graph processing.\n",
    "def _to_langchain_base_message(mlflow_message: ChatAgentMessage) -> BaseMessage:\n",
    "    if mlflow_message.role == \"user\":\n",
    "        return HumanMessage(content=mlflow_message.content)\n",
    "    elif mlflow_message.role == \"assistant\":\n",
    "        tool_calls = []\n",
    "        if mlflow_message.tool_calls:\n",
    "            # mlflow_message.tool_calls are already validated as mlflow.types.chat.ToolCall objects\n",
    "            # or dicts conforming to that, so we can access tc.function.name etc.\n",
    "            for tc in mlflow_message.tool_calls:\n",
    "                lc_args = {}\n",
    "                # Handle both dict and ToolCall object for 'tc'\n",
    "                if isinstance(tc, dict): # if it's a raw dict from MLflow's side\n",
    "                    if tc.get(\"function\") and tc[\"function\"].get(\"arguments\"):\n",
    "                        try:\n",
    "                            lc_args = json.loads(tc[\"function\"][\"arguments\"])\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Warning: Could not parse MLflow tool arguments as JSON: {tc['function']['arguments']}\")\n",
    "                            lc_args = {\"raw_arguments\": tc[\"function\"][\"arguments\"]}\n",
    "                    tool_calls.append({\n",
    "                        \"name\": tc[\"function\"][\"name\"],\n",
    "                        \"args\": lc_args,\n",
    "                        \"id\": tc[\"id\"]\n",
    "                    })\n",
    "                else: # assuming it's mlflow.types.chat.ToolCall object if not dict\n",
    "                    if tc.function and tc.function.arguments:\n",
    "                        try:\n",
    "                            lc_args = json.loads(tc.function.arguments)\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Warning: Could not parse MLflow tool arguments as JSON: {tc.function.arguments}\")\n",
    "                            lc_args = {\"raw_arguments\": tc.function.arguments}\n",
    "                    tool_calls.append({\n",
    "                        \"name\": tc.function.name,\n",
    "                        \"args\": lc_args,\n",
    "                        \"id\": tc.id\n",
    "                    })\n",
    "\n",
    "        return AIMessage(content=mlflow_message.content, tool_calls=tool_calls)\n",
    "    elif mlflow_message.role == \"tool\":\n",
    "        return ToolMessage(content=mlflow_message.content, name=mlflow_message.name, tool_call_id=mlflow_message.tool_call_id)\n",
    "    elif mlflow_message.role == \"system\":\n",
    "        return SystemMessage(content=mlflow_message.content)\n",
    "    else:\n",
    "        return BaseMessage(content=mlflow_message.content, type=mlflow_message.role)\n",
    "\n",
    "\n",
    "# --- _parse_dict_to_langchain_message remains unchanged (it converts raw graph dicts to LangChain BaseMessages) ---\n",
    "# This is crucial for the LangGraph's internal flow.\n",
    "def _parse_dict_to_langchain_message(msg_dict: dict) -> BaseMessage:\n",
    "    role = msg_dict.get(\"role\")\n",
    "    content = msg_dict.get(\"content\")\n",
    "    tool_calls_raw = msg_dict.get(\"tool_calls\")\n",
    "    name = msg_dict.get(\"name\") # For tool messages\n",
    "\n",
    "    if role == \"assistant\":\n",
    "        lc_tool_calls = []\n",
    "        if tool_calls_raw:\n",
    "            for tc_raw in tool_calls_raw:\n",
    "                tool_call_id = tc_raw.get(\"id\") \n",
    "                function_info = tc_raw.get(\"function\", {})\n",
    "                \n",
    "                args_dict = {}\n",
    "                raw_arguments_str = function_info.get(\"arguments\")\n",
    "                if raw_arguments_str:\n",
    "                    try:\n",
    "                        args_dict = json.loads(raw_arguments_str)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Could not parse tool arguments as JSON: {raw_arguments_str}\")\n",
    "                        args_dict = {\"raw_arguments\": raw_arguments_str}\n",
    "\n",
    "                lc_tool_calls.append({\n",
    "                    \"name\": function_info.get(\"name\"),\n",
    "                    \"args\": args_dict,\n",
    "                    \"id\": tool_call_id \n",
    "                })\n",
    "        return AIMessage(content=content, tool_calls=lc_tool_calls)\n",
    "    elif role == \"user\":\n",
    "        return HumanMessage(content=content)\n",
    "    elif role == \"tool\":\n",
    "        tool_call_id = msg_dict.get(\"tool_call_id\")\n",
    "        if not tool_call_id and msg_dict.get(\"id\"): \n",
    "            tool_call_id = msg_dict.get(\"id\")\n",
    "        return ToolMessage(content=content, name=name, tool_call_id=tool_call_id)\n",
    "    elif role == \"system\":\n",
    "        return SystemMessage(content=content)\n",
    "    else:\n",
    "        return BaseMessage(content=str(msg_dict), type=role)\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        \n",
    "        # Ensure that the last message is a LangChain AIMessage with tool_calls\n",
    "        # LangGraph's ToolNode expects this specific type\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: ([SystemMessage(content=system_prompt)] if system_prompt else [])\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    \n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    async def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        # The model_runnable will return a LangChain BaseMessage (e.g., AIMessage)\n",
    "        response = await model_runnable.ainvoke(state, config)\n",
    "        # Return the LangChain message directly within the state\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools)) \n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\", should_continue, {\"continue\": \"tools\", \"end\": END}\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    async def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        lc_messages = [_to_langchain_base_message(msg) for msg in messages]\n",
    "        request = {\"messages\": lc_messages}\n",
    "\n",
    "        response_messages = []\n",
    "        print(\"\\n--- Starting LangGraph Astream Debugging ---\")\n",
    "        print(f\"Initial Request Messages (LangChain format): {request['messages']}\")\n",
    "\n",
    "        async for event in self.agent.astream(request, stream_mode=\"updates\"):\n",
    "            print(f\"\\n--- LangGraph Event Received ---\")\n",
    "            print(f\"____________Raw Event Structure: {event}\") # Keep this for verbose debugging if needed\n",
    "\n",
    "            for node_name, node_output in event.items():\n",
    "                print(f\"  Node: {node_name}\")\n",
    "                print(f\"  ********** Node Output Data: {node_output}\") # Keep this for verbose debugging if needed\n",
    "\n",
    "                if node_output and node_output.get(\"messages\"):\n",
    "                    for raw_msg in node_output[\"messages\"]:\n",
    "                        # Convert dict to LangChain BaseMessage if it's not already one\n",
    "                        if isinstance(raw_msg, dict):\n",
    "                            lc_msg = _parse_dict_to_langchain_message(raw_msg)\n",
    "                            print(f\"    Converted raw dict to LangChain Message: {lc_msg}\")\n",
    "                        elif isinstance(raw_msg, BaseMessage):\n",
    "                            lc_msg = raw_msg\n",
    "                            print(f\"    Direct LangChain Message from node: {lc_msg}\")\n",
    "                        else:\n",
    "                            print(f\"    Skipping unknown message type in stream: {type(raw_msg)} - {raw_msg}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Apply your existing filtering and conversion to MLflow message\n",
    "                        if isinstance(lc_msg, AIMessage):\n",
    "                            print(f\"      AIMessage content: '{lc_msg.content}', tool_calls: {lc_msg.tool_calls}\")\n",
    "                            # Only add AIMessage if it has content OR tool_calls\n",
    "                            if lc_msg.content or lc_msg.tool_calls:\n",
    "                                mlflow_msg = _to_mlflow_chat_agent_message(lc_msg)\n",
    "                                print(f\"        Converted MLflow Message for response: {mlflow_msg}\")\n",
    "                                for toll_call in lc_msg.tool_calls:\n",
    "                                    # ToolCall(id='call_307a82b3-3568-4552-b579-02993ae0252c', type='function', function=Function(name='nimble_deep_web_search', arguments='{\"query\": \"safest areas in Texas\", \"num_results\": 5, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'))\n",
    "\n",
    "                                    repsonse_test= await self.agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the best Ethiopian restaurant on the northside of Chicago?\"}]})\n",
    "\n",
    "                                    print(f\"%%%%%%%%% repsonse: {repsonse_test}\")\n",
    "                                    \n",
    "\n",
    "                                response_messages.append(mlflow_msg)\n",
    "                            else:\n",
    "                                print(\"        Skipping empty AIMessage without tool calls.\")\n",
    "                        elif isinstance(lc_msg, HumanMessage):\n",
    "                            print(f\"      HumanMessage content: '{lc_msg.content}'\")\n",
    "                            mlflow_msg = _to_mlflow_chat_agent_message(lc_msg)\n",
    "                            print(f\"        Converted MLflow Message for response: {mlflow_msg}\")\n",
    "                            response_messages.append(mlflow_msg)\n",
    "                        elif isinstance(lc_msg, ToolMessage):\n",
    "                            print(f\"      ToolMessage content: '{lc_msg.content}', name: {lc_msg.name}\")\n",
    "                            mlflow_msg = _to_mlflow_chat_agent_message(lc_msg)\n",
    "                            print(f\"        Converted MLflow Message for response: {mlflow_msg}\")\n",
    "                            response_messages.append(mlflow_msg)\n",
    "                        else:\n",
    "                            print(f\"      Skipping non-displayable LangChain message type: {type(lc_msg)}\")\n",
    "\n",
    "        print(f\"\\n--- Final Assembled MLflow Response Messages: {response_messages} ---\")\n",
    "        return ChatAgentResponse(messages=response_messages)\n",
    "\n",
    "\n",
    "    async def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        lc_messages = [_to_langchain_base_message(msg) for msg in messages]\n",
    "        request = {\"messages\": lc_messages}\n",
    "\n",
    "        async for event in self.agent.astream(request, stream_mode=\"updates\"):\n",
    "            for node_name, node_output in event.items():\n",
    "                if node_output and node_output.get(\"messages\"):\n",
    "                    for raw_msg in node_output[\"messages\"]:\n",
    "                        if isinstance(raw_msg, dict):\n",
    "                            lc_msg = _parse_dict_to_langchain_message(raw_msg)\n",
    "                        elif isinstance(raw_msg, BaseMessage):\n",
    "                            lc_msg = raw_msg\n",
    "                        else:\n",
    "                            continue # Skip unknown types\n",
    "\n",
    "                        if isinstance(lc_msg, (AIMessage, HumanMessage, ToolMessage)):\n",
    "                            if isinstance(lc_msg, AIMessage) and not lc_msg.content and not lc_msg.tool_calls:\n",
    "                                continue # Skip empty AIMessages without tool calls\n",
    "                            yield ChatAgentChunk(**{\"delta\": _to_mlflow_chat_agent_message(lc_msg)})\n",
    "\n",
    "    # async def predict(\n",
    "    #     self,\n",
    "    #     messages: list[ChatAgentMessage],\n",
    "    #     context: Optional[ChatContext] = None,\n",
    "    #     custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    # ) -> ChatAgentResponse:\n",
    "    #     # Convert incoming MLflow ChatAgentMessages to LangChain BaseMessages\n",
    "    #     lc_messages = [_to_langchain_base_message(msg) for msg in messages]\n",
    "    #     request = {\"messages\": lc_messages}\n",
    "\n",
    "    #     response_messages = []\n",
    "    #     async for event in self.agent.astream(request, stream_mode=\"updates\"):\n",
    "    #         for node_data in event.values():\n",
    "    #             if node_data.get(\"messages\"):\n",
    "    #                 # node_data[\"messages\"] will now contain LangChain BaseMessage types\n",
    "    #                 # Convert them to MLflow ChatAgentMessage for the final response\n",
    "    #                 for lc_msg in node_data[\"messages\"]:\n",
    "    #                     # Only add if it's a message type we want to return\n",
    "    #                     # This avoids internal LangChain messages from appearing in the final output\n",
    "    #                     if isinstance(lc_msg, (AIMessage, HumanMessage, ToolMessage)):\n",
    "    #                          response_messages.append(_to_mlflow_chat_agent_message(lc_msg))\n",
    "        \n",
    "    #     # Filter for unique messages if the stream produces duplicates (LangGraph sometimes does)\n",
    "    #     # A simple way to deduplicate if needed, though not strictly necessary if graph logic is tight\n",
    "    #     # unique_response_messages = []\n",
    "    #     # seen_contents = set()\n",
    "    #     # for msg in response_messages:\n",
    "    #     #     if msg.content not in seen_contents:\n",
    "    #     #         unique_response_messages.append(msg)\n",
    "    #     #         seen_contents.add(msg.content)\n",
    "    #     # return ChatAgentResponse(messages=unique_response_messages)\n",
    "\n",
    "    #     print(f\"--- Final Response Messages: {response_messages} ---\")\n",
    "        \n",
    "    #     return ChatAgentResponse(messages=response_messages)\n",
    "\n",
    "\n",
    "    # async def predict_stream(\n",
    "    #     self,\n",
    "    #     messages: list[ChatAgentMessage],\n",
    "    #     context: Optional[ChatContext] = None,\n",
    "    #     custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    # ) -> Generator[ChatAgentChunk, None, None]:\n",
    "    #     # Convert incoming MLflow ChatAgentMessages to LangChain BaseMessages\n",
    "    #     lc_messages = [_to_langchain_base_message(msg) for msg in messages]\n",
    "    #     request = {\"messages\": lc_messages}\n",
    "\n",
    "    #     async for event in self.agent.astream(request, stream_mode=\"updates\"):\n",
    "    #         for node_data in event.values():\n",
    "    #             if node_data.get(\"messages\"):\n",
    "    #                 # node_data[\"messages\"] will contain LangChain BaseMessage types\n",
    "    #                 for lc_msg in node_data[\"messages\"]:\n",
    "    #                     # Only yield if it's a message type we want to stream\n",
    "    #                     if isinstance(lc_msg, (AIMessage, HumanMessage, ToolMessage)):\n",
    "    #                         yield ChatAgentChunk(**{\"delta\": _to_mlflow_chat_agent_message(lc_msg)})\n",
    "\n",
    "async def create_agent():\n",
    "    \"\"\"\n",
    "    An asynchronous function to initialize and return the agent.\n",
    "    This safely handles the async call to get tools.\n",
    "    \"\"\"\n",
    "    # LLM and system prompt setup\n",
    "    llm = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\")\n",
    "    \n",
    "    system_prompt = \"\"\n",
    "\n",
    "    # Initialize tools list\n",
    "    tools = []\n",
    "    uc_tool_names = [\"system.ai.python_exec\"]\n",
    "    uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "    tools.extend(uc_toolkit.tools)\n",
    "\n",
    "    # Nimble MCP Tools setup\n",
    "    nimble_client = MultiServerMCPClient({\n",
    "        \"nimble\": {\n",
    "            \"url\": \"https://mcp.nimbleway.com/sse\",\n",
    "            \"transport\": \"sse\",\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer 1c7cdd787c154fe3a78ebcc3428d82c48f4fada38e0042c6a427fbe7e3b842ae\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    nimble_tools = await nimble_client.get_tools()\n",
    "    tools.extend(nimble_tools)\n",
    "\n",
    "    # Create and compile the agent graph\n",
    "    compiled_agent_graph = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "    \n",
    "    # Instantiate our final agent object\n",
    "    agent = LangGraphChatAgent(compiled_agent_graph)\n",
    "    \n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a959a18-e164-4cea-8802-24cf49d7d427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a71dd8fa-daba-414b-81ad-2f01c4f6265e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-10ba32fd-730a-4c03-8a2f-f5f2b474fe00/lib/python3.11/site-packages/databricks/connect/session.py:454: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- Starting LangGraph Astream Debugging ---\nInitial Request Messages (LangChain format): [HumanMessage(content='What are safer areas in Texas?', additional_kwargs={}, response_metadata={})]\n\n--- LangGraph Event Received ---\n____________Raw Event Structure: {'agent': {'messages': [{'role': 'assistant', 'content': '', 'id': 'run--805cfa78-ccb3-4202-a134-8b9e8c4fe632-0', 'tool_calls': [{'id': 'call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', 'type': 'function', 'function': {'name': 'nimble_deep_web_search', 'arguments': '{\"query\": \"safest areas in Texas\", \"num_results\": 5, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'}}]}]}}\n  Node: agent\n  ********** Node Output Data: {'messages': [{'role': 'assistant', 'content': '', 'id': 'run--805cfa78-ccb3-4202-a134-8b9e8c4fe632-0', 'tool_calls': [{'id': 'call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', 'type': 'function', 'function': {'name': 'nimble_deep_web_search', 'arguments': '{\"query\": \"safest areas in Texas\", \"num_results\": 5, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'}}]}]}\n    Converted raw dict to LangChain Message: content='' additional_kwargs={} response_metadata={} tool_calls=[{'name': 'nimble_deep_web_search', 'args': {'query': 'safest areas in Texas', 'num_results': 5, 'search_engine': 'google_search', 'parsing_type': 'plain_text'}, 'id': 'call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', 'type': 'tool_call'}]\n      AIMessage content: '', tool_calls: [{'name': 'nimble_deep_web_search', 'args': {'query': 'safest areas in Texas', 'num_results': 5, 'search_engine': 'google_search', 'parsing_type': 'plain_text'}, 'id': 'call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', 'type': 'tool_call'}]\n        Converted MLflow Message for response: role='assistant' content='' name=None id='0e2e015e-992c-4a6f-9af0-f6bd3308b890' tool_calls=[ToolCall(id='call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', type='function', function=Function(name='nimble_deep_web_search', arguments='{\"query\": \"safest areas in Texas\", \"num_results\": 5, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'))] tool_call_id=None attachments=None\n%%%%%%%%% repsonse: {'messages': [{'role': 'user', 'content': 'What is the best Ethiopian restaurant on the northside of Chicago?', 'id': '142910b8-876b-4539-add4-ce7260cb5852'}, {'role': 'assistant', 'content': '', 'id': 'run--ae014363-e577-44f0-a076-3146c06397c3-0', 'tool_calls': [{'id': 'call_2c2dddcb-b3d1-4e68-a32a-2b141cc3f916', 'type': 'function', 'function': {'name': 'nimble_deep_web_search', 'arguments': '{\"query\": \"best Ethiopian restaurant northside Chicago\", \"num_results\": 1, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'}}]}]}\n\n--- Final Assembled MLflow Response Messages: [ChatAgentMessage(role='assistant', content='', name=None, id='0e2e015e-992c-4a6f-9af0-f6bd3308b890', tool_calls=[ToolCall(id='call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', type='function', function=Function(name='nimble_deep_web_search', arguments='{\"query\": \"safest areas in Texas\", \"num_results\": 5, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'))], tool_call_id=None, attachments=None)] ---\nmessages=[ChatAgentMessage(role='assistant', content='', name=None, id='0e2e015e-992c-4a6f-9af0-f6bd3308b890', tool_calls=[ToolCall(id='call_a762b06b-bbb7-4b69-af5d-dcb80b9272e4', type='function', function=Function(name='nimble_deep_web_search', arguments='{\"query\": \"safest areas in Texas\", \"num_results\": 5, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'))], tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    }
   ],
   "source": [
    "# # In your Databricks notebook cell\n",
    "# from mlflow.types.agent import (\n",
    "#     ChatAgentChunk,\n",
    "#     ChatAgentMessage,\n",
    "#     ChatAgentResponse,\n",
    "#     ChatContext,\n",
    "# )\n",
    "\n",
    "# # 1. Import the async factory function from your modified agent.py\n",
    "# from agent import create_agent\n",
    "\n",
    "# # 2. Await the function to get your fully initialized agent instance.\n",
    "# #    Databricks notebooks support top-level await.\n",
    "# AGENT = await create_agent()\n",
    "\n",
    "# response = AGENT.predict(messages=[ChatAgentMessage(role=\"user\", content=\"Can you look at Nimble MCP tool to get current weather risks in Texas?\")])\n",
    "\n",
    "# # 3. Now you can use the agent with its synchronous predict method as before.\n",
    "# # response = AGENT.predict(messages=[ChatAgentMessage(role=\"user\", content=\"Hello! What can you do?\")])\n",
    "\n",
    "# print(response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In your Databricks notebook cell\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "\n",
    "# 1. Import the async factory function from your modified agent.py\n",
    "from agent import create_agent\n",
    "\n",
    "# 2. Await the function to get your fully initialized agent instance.\n",
    "#    Databricks notebooks support top-level await.\n",
    "AGENT = await create_agent()\n",
    "\n",
    "# 3. AWAIT THE PREDICT METHOD SINCE IT IS NOW ASYNC\n",
    "# response = await AGENT.predict(messages=[ChatAgentMessage(role=\"user\", content=\"Can you look at Nimble MCP tool to get current weather risks in Texas?\")])\n",
    "\n",
    "\n",
    "response = await AGENT.predict(messages=[ChatAgentMessage(role=\"user\", content=\"What are safer areas in Texas?\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5c3460-6747-48b3-9242-3701c239a9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[ChatAgentMessage(role='assistant', content='', name=None, id='f976f207-dce7-4636-8ed1-4ae76ff0c5d5', tool_calls=[ToolCall(id='call_00ce73e2-cd59-4b38-9a73-f1d518649486', type='function', function=Function(name='nimble_deep_web_search', arguments='{\"query\": \"Texas current weather risks\", \"num_results\": 3, \"search_engine\": \"google_search\", \"parsing_type\": \"plain_text\"}'))], tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    }
   ],
   "source": [
    "print(response)\n",
    "\n",
    "\n",
    "# # response = AGENT.predict(messages=[ChatAgentMessage(role=\"user\", content=\"Can you look at Nimble MCP tool to get current weather risks in Texas?\")])\n",
    "\n",
    "# for event in AGENT.predict_stream(\n",
    "#     {\"messages\": [ChatAgentMessage(role=\"user\", content=\"Can you look at Nimble MCP tool to get current weather risks in Texas?\")]}\n",
    "# ):\n",
    "#     print(event, \"-----------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec1b7fc-dfab-419a-a9fe-c43f7489b696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[ChatAgentMessage(role='assistant', content=\"Hello! I'm here to help you with various tasks and answer your questions. I can process and generate text, perform calculations, and even interact with external tools to provide more accurate and helpful responses. Here are some examples of what I can do:\\n\\n1. **Text-based tasks**: I can understand and respond to natural language queries, summarize content, translate text, and even generate text based on a prompt.\\n2. **Problem-solving**: I can help with mathematical calculations, provide step-by-step solutions to problems, and offer explanations for complex concepts.\\n3. **Information retrieval**: I can search for information on the web, retrieve data from various sources, and provide answers to factual questions.\\n4. **Tool-based tasks**: I can interact with various tools to perform tasks such as data extraction, web scraping, and more.\\n\\nSome specific tools I can use include:\\n* `nimble_targeted_engines`: Fetch lists of available Nimble Web Agent templates for targeted data extraction.\\n* `nimble_targeted_retrieval`: Execute data extraction using Nimble Web Agent's pre-trained templates.\\n* `nimble_google_maps_search`: Perform a Google Maps search using Nimble's SERP.\\n* `nimble_google_maps_reviews`: Collect reviews for a specific place from Google Maps using Nimble's API.\\n* `nimble_google_maps_place`: Retrieve detailed information about a place from Google Maps using Nimble's API.\\n* `nimble_effortless_pipelines`: Get Nimble Effortless Pipeline definitions.\\n* `nimble_effortless_templates`: Get Nimble Effortless Pipeline templates.\\n* `nimble_deep_web_search`: Perform a web search using Nimble's Search API.\\n* `nimble_extract`: Extract content from a specific URL using Nimble's Extract API.\\n\\nWhat would you like me to help you with today?\", name=None, id='run--607753b8-a3b8-40cb-9e14-c2bd920bff09-0', tool_calls=None, tool_call_id=None, attachments=None)] finish_reason=None custom_outputs=None usage=None\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "953ed877-7016-4581-826a-7bf13e319cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "deployable_agent_draft__nonworking_draft",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}